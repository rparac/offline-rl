"""This module wraps torch.multiprocessing.spawn to automatically pass stop events."""
"""Borrowed from https://github.com/AlignmentResearch/vlmrm/blob/main/src/vlmrm/multiprocessing.py"""

from torch import multiprocessing
from torch.multiprocessing.spawn import ProcessContext, _prctl_pr_set_pdeathsig
import signal
import sys
import warnings
import tempfile
import os
import pickle


def _wrap(fn, i, args, error_file, stop_event):
    # prctl(2) is a Linux specific system call.
    # On other systems the following function call has no effect.
    # This is set to ensure that non-daemonic child processes can
    # terminate if their parent terminates before they do.
    _prctl_pr_set_pdeathsig(signal.SIGINT)

    try:
        fn(i, stop_event, *args)
    except KeyboardInterrupt:
        # SIGINT; killed by parent, do nothing
        pass
    except Exception:
        # Propagate exception to parent process, keeping original traceback
        import traceback

        # Write a pickled traceback string so that ProcessContext.join()
        # can unpickle it via pickle.load(fh).
        with open(error_file, "wb") as f:
            pickle.dump(traceback.format_exc(), f)
        sys.exit(1)


# Note: [start_processes]
# mp.start_processes handles both start_method='spawn' and 'fork'. It's supposed to be a
# more generalized API than mp.spawn. Currently we only document mp.spawn as it's the
# CUDA compatible start_method. However, in environments like Ipython notebooks, 'fork'
# works better than 'spawn'. Every helper function we created for mp.spawn is indeed
# general enough, and backends like XLA can reuse them in Colab notebooks as well.
# Currently we only add this API first, we can consider adding it to documentation as
# needed in the future.

def start_processes(
    fn, args=(), nprocs=1, join=True, daemon=False, start_method="spawn"
):
    mp = multiprocessing.get_context(start_method)
    stop_event = mp.Event()
    error_files = []
    processes = []

    for i in range(nprocs):
        # Create a temporary file for error reporting that ProcessContext
        # will later read with pickle.load().
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tmp.close()
        error_files.append(tmp.name)

        process = mp.Process(
            target=_wrap,
            args=(fn, i, args, tmp.name, stop_event),
            daemon=daemon,
        )
        process.start()
        processes.append(process)

    context = ProcessContext(processes, error_files)

    if not join:
        # In this case the caller will drive context.join() themselves.
        # We don't eagerly delete error files here, since ProcessContext
        # still needs them if a child errors later.
        return context

    try:
        # Loop on join until it returns True or raises an exception.
        while not context.join():
            pass
    finally:
        # After all processes are done and any errors have been read,
        # remove the temporary error files we created.
        for path in error_files:
            try:
                if os.path.exists(path):
                    os.unlink(path)
            except OSError:
                # Best-effort cleanup only.
                pass


def spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method="spawn"):
    r"""Spawns ``nprocs`` processes that run ``fn`` with ``args``.

    If one of the processes exits with a non-zero exit status, the
    remaining processes are killed and an exception is raised with the
    cause of termination. In the case an exception was caught in the
    child process, it is forwarded and its traceback is included in
    the exception raised in the parent process.

    Args:
        fn (function): Function is called as the entrypoint of the
            spawned process. This function must be defined at the top
            level of a module so it can be pickled and spawned. This
            is a requirement imposed by multiprocessing.

            The function is called as ``fn(i, *args)``, where ``i`` is
            the process index and ``args`` is the passed through tuple
            of arguments.

        args (tuple): Arguments passed to ``fn``.
        nprocs (int): Number of processes to spawn.
        join (bool): Perform a blocking join on all processes.
        daemon (bool): The spawned processes' daemon flag. If set to True,
                       daemonic processes will be created.
        start_method (str): (deprecated) this method will always use ``spawn``
                               as the start method. To use a different start method
                               use ``start_processes()``.

    Returns:
        None if ``join`` is ``True``,
        :class:`~ProcessContext` if ``join`` is ``False``

    """
    if start_method != "spawn":
        msg = (
            "This method only supports start_method=spawn (got: %s).\n"
            "To use a different start_method use:\n\t\t"
            " torch.multiprocessing.start_processes(...)" % start_method
        )
        warnings.warn(msg)
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
